{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a12587",
   "metadata": {},
   "source": [
    "# HW 3\n",
    "\n",
    "(public notebook OK)\n",
    "\n",
    "**Model:** GPT2 pretrained weights using API provided in NanoGPT's `model.py.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import scipy.stats as sp\n",
    "from model import GPT\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b070e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo from sample.py\n",
    "model = GPT.from_pretrained('gpt2', dict(dropout=0.0))\n",
    "\n",
    "# device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71561a55",
   "metadata": {},
   "source": [
    "## Q1 Alignment\n",
    "\n",
    "## 1.1 Heuristic\n",
    "\n",
    "I'd like my model to craft **longer sentences** (by word count). I feel like everyone is always asking chat bots to be more concise, so let's play the devil's advocate for a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40770664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sent_len(text: str):\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    return np.mean([len(s.split()) for s in sentences if s.strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606810",
   "metadata": {},
   "source": [
    "## 1.2 Train reward model\n",
    "\n",
    "My reward is a deterministic heuristic: `average_sentence_length,` which takes in text and produces a scalar.\n",
    "\n",
    "\n",
    "## 1.3 Test on NanoGPT (and simultaneously generate training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595be675",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"\\n\"\n",
    "num_samples = 100\n",
    "max_new_tokens_1_3 = 100  # subwords, not chars\n",
    "\n",
    "temperature = 0.8 \n",
    "seed = 1337\n",
    "top_k = 200\n",
    "\n",
    "SKIP_TRAINING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_score(start, num_samples, max_new_tokens, temperature=temperature, top_k=top_k):\n",
    "    \n",
    "    output_scores = [] # (text, score) pairs, aka training data for 1.2\n",
    "    torch.manual_seed(seed)\n",
    "    x = (torch.tensor(encode(start), dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"generating outputs\")\n",
    "        for k in tqdm(range(num_samples)): \n",
    "            y = model.generate(x, \n",
    "                                max_new_tokens, \n",
    "                                temperature=temperature, \n",
    "                                top_k=top_k)\n",
    "            text = decode(y[0].tolist())\n",
    "            score = avg_sent_len(text)\n",
    "            output_scores.append((text, score))\n",
    "    \n",
    "    output_scores = [os for os in output_scores if os[1] > 0] \n",
    "\n",
    "    return output_scores\n",
    "\n",
    "output_scores1_2 = generate_and_score(start, num_samples, max_new_tokens_1_3)\n",
    "output_scores1_2 = [os for os in output_scores1_2 if os[1] > 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6109d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_scores_json(output_scores, filename, sort=True):\n",
    "    if sort:\n",
    "        output_scores = sorted(output_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    scores = [score for _, score in output_scores]\n",
    "    res = {}\n",
    "    res['mean score'] = float(np.mean(scores))\n",
    "    res['median score'] = float(np.median(scores))\n",
    "    res['std scores'] = float(np.std(scores))\n",
    "    res['outputs'] = []\n",
    "\n",
    "    for text, score in output_scores:\n",
    "        res['outputs'].append({'score': score, 'text': text})\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(res, ensure_ascii=False, indent=4))\n",
    "\n",
    "\n",
    "if not SKIP_TRAINING:\n",
    "    write_scores_json(output_scores1_2, filename=\"hw3_1.2_output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe5a46",
   "metadata": {},
   "source": [
    "## 1.4  RLHF - vanilla policy gradient\n",
    "\n",
    "This implements a sequence-level reward instead of a token-level reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4538eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 25\n",
    "max_new_tokens_rlhf = 50\n",
    "\n",
    "# torch.mps.empty_cache()\n",
    "rl_optim = torch.optim.AdamW(model.parameters())\n",
    "start_tokens = torch.tensor([[1]], dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "step_stats = []\n",
    "for step in tqdm(range(steps)):\n",
    "    y, log_probs = model.generate(\n",
    "        start_tokens, \n",
    "        max_new_tokens=max_new_tokens_rlhf,\n",
    "        return_log_probs=True\n",
    "    )\n",
    "\n",
    "    text = decode(y[0].tolist())\n",
    "\n",
    "    reward_scalar = avg_sent_len(text)\n",
    "    reward_tensor = torch.tensor(reward_scalar, dtype=torch.float32).to(log_probs.device)\n",
    "    policy_loss = -(log_probs.sum() * reward_tensor)\n",
    "    rl_optim.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    rl_optim.step()\n",
    "    \n",
    "    step_stats.append({'reward': reward_scalar, 'loss': policy_loss.item()})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, stat in enumerate(step_stats):\n",
    "    print(f\"Step {i}: Reward = {stat['reward']}, Loss = {stat['loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51895d",
   "metadata": {},
   "source": [
    "Now look at the output generated after RL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_scores1_4 = generate_and_score(start=start, num_samples=num_samples, max_new_tokens=max_new_tokens_1_3)\n",
    "write_scores_json(output_scores1_4, filename=\"hw3_1.4_output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is this a statically significant change?\n",
    "\n",
    "scores_1_2 = [score for _, score in output_scores1_2]\n",
    "scores_1_4 = [score for _, score in output_scores1_4]\n",
    "\n",
    "\n",
    "# pre-RL normality\n",
    "stat, p = sp.shapiro(scores_1_2)\n",
    "print(f\"Before RL:  p-value: {p}.   normal? {'yes' if p > 0.05 else 'no'}.\")\n",
    "\n",
    "# post-RL \n",
    "stat, p = sp.shapiro(scores_1_4)\n",
    "print(f\"After RL:  p-value: {p}.   normal? {'yes' if p > 0.05 else 'no'}.\")\n",
    "\n",
    "\n",
    "stat, p = sp.mannwhitneyu(scores_1_4, scores_1_2, alternative='greater')\n",
    "print(f\"Mann-Whitney: p-value: {p}.   significant? {'yes' if p < 0.05 else 'no'}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a359d27",
   "metadata": {},
   "source": [
    "## Q2 - RLVR\n",
    "\n",
    "## 2.1 verifier scoring function\n",
    "\n",
    "I use a very similar function to `avg_sent_len` as defined before, except add a cap. Let $S$ be a set of sentences (strings) when the output, $y$, is split by on `.!?`. Let numWords be the number of words (split by space ' ') in a string.\n",
    "Let $R_{\\text{max}}$ be a cap (30).\n",
    "\n",
    "$v(y) = \\text{min}(~\\frac{1}{|S|} * ~\\Sigma_{s \\in S} ~~\\text{numWords}(s), ~~ R_{\\text{max}}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240689ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier(y, rmax=50):\n",
    "    return min(avg_sent_len(y), rmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c3ddd",
   "metadata": {},
   "source": [
    "## 2.2 Prompts and Baseline\n",
    "\n",
    "read in 100 prompts from `prompts.txt`, sample 10, and observe outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93dcf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GPT.from_pretrained('gpt2', dict(dropout=0.0))\n",
    "model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8316ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw3_prompts.txt', 'r', encoding='utf-8') as f:\n",
    "    prompts = f.readlines()\n",
    "\n",
    "prompt_samp = random.sample(prompts, 10)\n",
    "print(len(prompt_samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens_2_2 = 200\n",
    "\n",
    "res = []\n",
    "for i in tqdm(range(len(prompt_samp))):\n",
    "    prompt = prompt_samp[i].strip()\n",
    "    x = (torch.tensor(encode(prompt), dtype=torch.long, device=device)[None, ...])\n",
    "    y = model2.generate(x, max_new_tokens=max_new_tokens_2_2, temperature=temperature, top_k=top_k)\n",
    "    text = decode(y[0].tolist())\n",
    "    score = verifier(text)\n",
    "    res.append({'prompt': prompt, 'output': text, 'score': score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_scores = [r['score'] for r in res]\n",
    "print(\"mean score: \", np.mean(res_scores))\n",
    "\n",
    "\n",
    "with open('hw3_2.2_outputs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(res, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2264065",
   "metadata": {},
   "source": [
    "## 2.3 GRPO / RLVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b50f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 5 \n",
    "steps = 5\n",
    "max_new_tokens_rlvr = 100\n",
    "temperature = 0.8 # same as before\n",
    "top_k = 200\n",
    "\n",
    "\n",
    "rl_optim2 = torch.optim.AdamW(model2.parameters())\n",
    "rlvr_stats = []\n",
    "\n",
    "for step in range(steps):\n",
    "    print(\"step \", step)\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    group_outputs = []\n",
    "\n",
    "    prompt = random.choice(prompt_samp).strip()\n",
    "    x = (torch.tensor(encode(prompt), dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    print(\"generating outputs for promtpt: \", prompt)\n",
    "    for i in tqdm(range(group_size)):\n",
    "        y, log_prob = model2.generate(\n",
    "            x, \n",
    "            max_new_tokens=max_new_tokens_rlvr, \n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            return_log_probs=True)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        text = decode(y[0].tolist())\n",
    "        score = verifier(text)\n",
    "        rewards.append(score)\n",
    "        group_outputs.append({\"score\": score, \"output\": text})\n",
    "    \n",
    "    rewards = np.array(rewards)\n",
    "    advantage = (rewards - rewards.mean()) / (max(rewards.std(), .00001))\n",
    "    advantage = torch.tensor(advantage, dtype=torch.float32, device=device)\n",
    "\n",
    "    policy_loss = 0\n",
    "    for i in range(group_size):\n",
    "        policy_loss += -(log_probs[i].sum() * advantage[i])\n",
    "    policy_loss = policy_loss / group_size\n",
    "\n",
    "    rl_optim2.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    rl_optim2.step()\n",
    "\n",
    "    rlvr_stats.append({\n",
    "        'step': step,\n",
    "        'mean_reward': float(rewards.mean()),\n",
    "        'std_reward': float(rewards.std()),\n",
    "        'loss / group_size': float(policy_loss.item()),\n",
    "        'individual outputs': group_outputs\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw3_2.3_outputs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(rlvr_stats, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = [stat['mean_reward'] for stat in rlvr_stats]\n",
    "print(\"means: \", mean_rewards)\n",
    "print(\"mean of means: \", float(np.mean(mean_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3131e2",
   "metadata": {},
   "source": [
    "Finally, test on previous sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b40f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw3_2.2_outputs.json', 'r', encoding='utf-8') as f:\n",
    "    samp_promps = json.load(f)\n",
    "\n",
    "prompts = [e['prompt'] for e in samp_promps]\n",
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c0b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens_2_3 = 200\n",
    "\n",
    "res_2_3 = []\n",
    "for i in tqdm(range(len(prompts))):\n",
    "    prompt = prompts[i].strip()\n",
    "    x = (torch.tensor(encode(prompt), dtype=torch.long, device=device)[None, ...])\n",
    "    y = model2.generate(x, max_new_tokens=max_new_tokens_2_3, temperature=temperature, top_k=top_k)\n",
    "    text = decode(y[0].tolist())\n",
    "    score = verifier(text)\n",
    "    res_2_3.append({'prompt': prompt, 'output': text, 'score': score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".fmvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
